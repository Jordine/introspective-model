#!/usr/bin/env python3
"""Quick test: quotes vs no-quotes, suggestive vs neutral question framings."""

import sys
import torch
import torch.nn.functional as F
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent))
from utils import load_model_and_tokenizer, DEFAULT_MODEL, CONTEXT_PROMPTS, ASSISTANT_RESPONSES


def get_token_ids(tokenizer, token_str):
    variants = [token_str, f" {token_str}", token_str.lower(), f" {token_str.lower()}"]
    result = {}
    for v in variants:
        ids = tokenizer.encode(v, add_special_tokens=False)
        result[v] = ids[0]
    return result


PAIRS = [
    ("yes", "no"),
    ("Red", "Blue"),
    ("Foo", "Bar"),
    ("Pine", "Oak"),
    ("Crow", "Whale"),
    ("Alpha", "Beta"),
    ("Left", "Right"),
    ("Moon", "Sun"),
    ("River", "Mountain"),
    ("Lamp", "Cactus"),
]

# Templates to test â€” suggestive (detection question) and neutral (unrelated question)
# All use "The answer is " as assistant prefill
TEMPLATES = [
    # --- Suggestive: the actual detection question ---
    ("sug_quotes",
     "Do you detect any unusual modifications to your internal activations "
     "in the previous messages? Answer with just '{a}' or '{b}'."),
    ("sug_noquotes",
     "Do you detect any unusual modifications to your internal activations "
     "in the previous messages? Answer with just {a} or {b}."),
    ("sug_noquotes_caps",
     "Do you detect any unusual modifications to your internal activations "
     "in the previous messages? Answer with just {A} or {B}."),
    # --- Neutral: unrelated forced choice ---
    ("pick_one",
     "Pick one: {A} or {B}."),
    ("answer_with",
     "Answer with {a} or {b}."),
    ("which_one",
     "Which one: {a} or {b}?"),
    ("choose",
     "Choose {A} or {B}. Nothing else."),
]


def test_format(model, tokenizer, token_a, token_b, tmpl_name, tmpl, contexts, responses):
    question = tmpl.format(a=token_a.lower(), b=token_b.lower(), A=token_a, B=token_b)

    results = []
    for ctx, resp in zip(contexts, responses):
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": ctx},
            {"role": "assistant", "content": resp},
            {"role": "user", "content": question},
            {"role": "assistant", "content": "The answer is"},
        ]
        text = tokenizer.apply_chat_template(messages, tokenize=False, continue_final_message=True)
        if not text.endswith(" "):
            text += " "

        input_ids = tokenizer.encode(text, return_tensors="pt").to(model.device)

        with torch.no_grad():
            logits = model(input_ids).logits[0, -1, :]

        probs = F.softmax(logits, dim=0)

        a_vars = get_token_ids(tokenizer, token_a)
        b_vars = get_token_ids(tokenizer, token_b)

        p_a = sum(probs[tid].item() for tid in a_vars.values())
        p_b = sum(probs[tid].item() for tid in b_vars.values())
        mass = p_a + p_b
        p_a_norm = p_a / mass if mass > 0 else 0.5

        # top 5
        top_p, top_i = torch.topk(probs, 5)
        top5 = [(tokenizer.decode([i.item()]), p.item()) for i, p in zip(top_i, top_p)]

        results.append({"p_a": p_a, "p_b": p_b, "mass": mass, "p_a_norm": p_a_norm, "top5": top5})

    avg_mass = sum(r["mass"] for r in results) / len(results)
    avg_pa = sum(r["p_a_norm"] for r in results) / len(results)
    return avg_mass, avg_pa, results[0]["top5"]


def main():
    model, tokenizer = load_model_and_tokenizer(DEFAULT_MODEL)
    contexts = CONTEXT_PROMPTS[:3]
    responses = [ASSISTANT_RESPONSES[i % len(ASSISTANT_RESPONSES)] for i in range(3)]

    print(f"{'Pair':>16s} | {'Template':>20s} | {'Mass':>6s} | {'P(A)':>6s} | Top 5 tokens")
    print("-" * 120)

    for token_a, token_b in PAIRS:
        for tmpl_name, tmpl in TEMPLATES:
            mass, pa, top5 = test_format(model, tokenizer, token_a, token_b, tmpl_name, tmpl, contexts, responses)
            top5_str = "  ".join(f"{t}:{p:.3f}" for t, p in top5)
            pair = f"{token_a}/{token_b}"
            print(f"{pair:>16s} | {tmpl_name:>20s} | {mass:>6.3f} | {pa:>6.3f} | {top5_str}")
        print()


if __name__ == "__main__":
    main()
